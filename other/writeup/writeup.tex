\documentclass[a4paper, 12pt]{article}
\usepackage{mathtools}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\setlength{\parskip}{1em}

\begin{document}

\title{Non-photorealistic Rendering Techniques}
\author{Gabrielius Slakaitis}
\date{\today}
\maketitle

\newpage
\section{Silhouette Shading}
The technique of silhouette shading aims to achieve two effects. The first is to set a boundary between the area that is shaded and the blank page, highlighting details that may be hard to spot due to the two-dimensionality of the image. The second is to provide a communication of lighting - render bold lines in the shadowed sections of the silhouette and thin lines in the light. This will not only provide some necessary information about the scene, but also give the illusion of an artist's touch due to the varying line weights.

Most of the current methods for silhouette shading are image-based and rely on buffers to extract features. While this works for most applications, it would be difficult to flexibly vary the line width using this method while making sure the techniques work on low-poly objects*. Geometric methods can also be used to distinguish features by selecting edges on the silhouette, however making the CPU characterize these edges every frame, and passing the correct ones for the GPU to render would quickly become a bottleneck. A way to tackle this problem is to pass adjacent face information about every edge to the GPU and allow it to cull the unwanted ones, which is the heart of the algorithm. In this paper I have extended this basic idea, and used the passed information to calculate unique lighting properties of the silhouette edges to achieve the desired effect.

% add an explanation of which edges will be culled by this stage or in a seperate section

\subsection{Preprocessing}
% by this point need to have an idea why we could need the edge coordinates, face normals and vertex normals for the first two sentences
% possibly move towards the end
In order to pass the required information to the GPU, a list of all edges in the mesh needs to be constructed. Neighbouring face normals and vertex normals should also be known for each edge. However, most generic meshes usually only store the coordinates of each vertex, followed by a list of faces represented as groups of these vertices. It is also a convention for every face to have its own version of a vertex, which leads to a single corner of a model often having three or more vertices associated with it. This is a problem as in this case the vertex normals cannot be calculated by averaging the normals of faces that include it, and finding unique edges becomes a challenge. The goal of this preprocessing step is to extract this necessary information from any given mesh before any rendering is done.

In all standard formats we can easily iterate through the faces of the model and access each of the vertices associated with it, and hence and calculate the face normal. For uniqueness, this step stores discovered vertices only by their coordinates, and edges by the coordinates of their end-vertices in lexical order. One unordered map structure keeps key-value pairs with vertices as keys and their accumulating vertex normals as values. If a new vertex is discovered while iterating through the faces, a new pair is added of the vertex and the current face normal. If the vertex is already in the map, then the current face normal is added to the currently stored value of the pair. Clearly after every face is visited this map will contain the sum of adjacent edge normals at every vertex, which can be normalized to get the vertex normal. A second unordered map contains edges and their neighbouring face normals which respectively make up the key-value pairs. If a new edge is discovered, then a pair of the edge and the current face normal is added. If an edge is already in the map, then both its neighbouring face normals can be accessed and grouped together, since an edge can only have at most two neighbours. By the end of this process all edges and their neighbouring faces can be accessed, and the vertex normals looked up via the first map.

Even if this stage only happens at the start of the program, a very large number of vertices may be processed, so an unordered map structure is used to reduce the loading time. This means writing and accessing of elements are $O(1)$ operations, making the whole preprocessing stage run in $O(\abs{faces})$ time. In practice this happens almost instantly for thousands of faces.

\subsection{Rendering the edge}
% possibly move this to 'geometric' section at the top
Using line primitives in OpenGL leads to uncertainty. In some cases using a particular line width might not be supported by an implementation and if this happens, the closest supported width would be used. In fact most graphics APIs do not guarantee the ability to rasterize lines of any width other than one pixel. This means constructing the edges from triangles is the only reliable option as the exact dimensions can be accurately controlled. In addition, line joining is not supported, which leads to artifacts as shown in [figure] when drawing thick lines. There are several styles of line joins that could be used to fix this by adding caps between two lines [source]. However, they all rely on some knowledge about both the edges, and the GPU will not have this information since processing of each vertex is independent. To counter this problem I will modify and extend an algorithm described in [source], which uses the vertex normal - the only mutual information available between edges - to generate edge caps.

% possibly move this to 'geometric' section at the top
Six points are required in order to draw an edge. The first four form the main quad: with two points at the base of the line and the other two extruding perpendicular to the direction of the line. Since the silhouette of the object should be drawn on the exterior side*, the quad must extrude away from the surface of the mesh. This can be ensured by checking that the direction of the edge normal and the perpendicular vector point in the same direction. The last two points are used in order to draw the caps on either side and extend along the direction of the vertex normal. Since any two adjacent edges will share this vertex and its normal, this will be a common point between them, so long as they are extended by the same amount. In any of these cases the magnitude of extrusion will depend on the lighting calculations.

The required attributes for every vertex passed into the shader are ($v$, $v'$, $n$, $n_A$, $n_B$, $i$) where $v$, $v'$ are the vertices at the base of the edge; $n$ is the vertex normal; $n_A$, $n_B$ are the adjacent face normals and $i$ is an integer between 0 and 2 determining whether this vertex represents the base, the roof or the cap of the edge. Note that $i$ only represents the type of the vertex, whereas which end of the edge it will be on is determined by which end the $v$ and the corresponding $n$ are.

The first thing that the shader has to work out is the weight of the line. Usually the intuitive way to calculate the lighting is to use the face normal: the further away from the light a face is, the darker it will be drawn. In this case an edge sits in the middle between two faces, so it is natural to take the average of the face normals - call this the edge normal. Then the standard calculation can be applied: $w = light \cdot normal$. Parameters can be used to take this value from [-1, 1] to the desired range.

Each one of the six vertices passed into the shader will represent a point in [figure]. The first step of the shader is to calculate the screen coordinates of  $v$, $v'$ and the vector $n$. These will be represented as $s$, $s'$ and $m$. If the type of the vertex is the base of the edge, $i=0$, then the final position is simply $s$. Otherwise the shader calculates the line direction vector $s'-s$, and its perpendicular - $p$, found by swapping the x and y coordinates of the direction vector and then negating the x coordinate. This perpendicular is normalized and multiplied by the weight of the line $w$, which makes its length $w$. If the type of the vertex is the roof, $i=1$, then the final position is $s+p$. Finally, if the vertex is the cap, $i=2$, the vector $m$ is lengthened to $w_v$ in a similar fashion and the final position becomes $s+v$.

One slightly tedious consideration concerns the fact that the screen coordinates range from -1 to 1 for both x and y coordinates, no matter the aspect ratio. The ratio itself is therefore also passed into the shader and the screen coordinates are modified accordingly.

Edges should only be visible if they are on the silhouette of the shape. This is characterized by the case where only a single one of the faces can be seen from the eye position. Formally this is represented as the following formula:
$$[n_A \cdot (eye-v) < 0] \oplus [n_B \cdot (eye-v) < 0]$$
Note that it does not matter whether $v$ or $v'$ are used in this formula, so this will not cause an issue where only one side of the edge is clipped. If this constraint is not satisfied by the attributes of a vertex, it means that they represent an edge which shouldn't be seen. Unfortunately the vertex shader is unable to directly destroy such geometry, but its position can still be changed. In particular, the vertices of such and edge will be transformed behind the clipping plane. Simply put, we're taking all the edges that shouldn't be seen and moving them behind the camera, where they will be discarded. Since clipping happens before rasterization in the graphics pipeline, these vertices will be discarded early and not cost much processing power.

When all the positions of the vertices are known, triangles can be rendered. The vertices describing every edge will be passed into the vertex shader as one long list - every six vertices defining one edge. On top of that, information has to be given about where to draw the triangles between the vertices. This is given by an index list, where every three indices represent a triangle that is rendered. For each edge i in the vertex list (which has vertex indices ranging from 6i to 6i+5) a 12 index list section is defined that describes these four triangles: [6i, 6i+2, 6i+3, 6i, 6i+3, 6i+1, 6i, 6i+4, 6i+2, 6i+1, 6i+3, 6i+5]. This pattern is repeated for every edge.

\subsection{Alternative solutions}

The algorithm for construction edges and caps in the given way is not perfect in all scenarios. A particular edge of the model has a constant width from one side to the other. This is exactly what is required when drawing low poly objects where straight lines do in fact represent a flat surface, but not always satisfactory when trying to represent a smooth curve seen in [figure].

\paragraph{Vertex-based weights.}
This is a direct translation of the Gouraud shading method, which is usually used to render faces by calculating the colour value at each vertex and interpolating across. Similarly, this alternate method uses the weight value calculated using the vertex normals and extending the roof of the edge accordingly. The result is shown in [figure].

Going further in the same direction leads to an easy optimisation. Since in this method the cap vertex and the roof vertex extend from the base by the same amount we can  completely get rid of the roof vertices and  construct the whole edge out of just the four points. Understandably, due to the fact that the vector n is not perpendicular to the edge, the width of the line will be lower as compared to having the roof. In practice this is not a problem - especially since this method will be used when the shape is smooth, and the angle between n and p low.

While this is a great improvement, there is still an issue present in this result relating to smooth surfaces. Each edge in the mesh has some weight value attached to it, depending on its direction to the light. The result of edge culling means that we can only see an edge if it's on the very silhouette of the mesh. Rotating around an object, at some point in time the current edge on the silhouette would be completely hidden behind a face and a new edge would emerge as the silhouette. Since this edge would have a distinct value to the previous one, there would be a point in time when the silhouette jumps in width. Clearly this is an issue for the original method too. Once again, this could be the desired effect when considering low-poly objects, but not when we want something to appear smooth. This artifact is also seen in still images when there's added curvature in the mesh, as in [figure]. At certain points a sudden switch occurs between the light value of the edges on this curved mesh. This artifact strongly depends on the model shape, how its lines are organised, the point of view and the position of the light, but it still breaks continuity in some cases.

\paragraph{Tangent-based weights.}
In order to make the edges independent from their weight, edge normals or vertex normals cannot be used for the lighting calculation. Instead, the position of the eye and in relation to the edge proves to be helpful. The vector describing the gaze from the eye to a particular point of the mesh can be easily found. Imagining the surface of the mesh to be infinitely smooth, no matter the position of the eye, the normal vector on the silhouette will be exactly perpendicular to the this gaze. To fully define this normal in three dimensions it needs another parameter. The direction of the edge itself can be used for this, since the normal has to be perpendicular to this edge. Now we can express this normal formally:
$$(v-eye) \times (v-v_2)$$
This indeed solves the artifact, however the problem of every edge having a uniform weight throughout its length reappears.  This leads to the same steps on the curve as seen in the original method [figure]. Sadly this problem is not easily fixed. It would be intuitive to use the vertex normals in order to calculate this perpendicular instead of the direction of the edge. In fact, the calculation for the vector would then become
$$(v-eye) \times [n x (v-eye)]$$
rewritten to reduce shader computation as
$$[(v-eye) \cdot (v-eye)]n - [(v-eye) \cdot v](v-eye)$$
However this brings back the dependency of the vertex normals and the original artifact comes back, rendering the approach useless.

\paragraph{Hybrid weights.}
In reality a single object might contain one section which has sharp corners and another which is simulating a smooth surface. In this case an additional parameter can be passed that determines the type of shading that should be applied. Since this parameter would be on a per-vertex basis the transition between these types would be smooth.

Both of the vertex-based and tangent-based methods would eventually lead to perfect transitioning as the detail of the mesh tends towards infinity. In practice, the artifact caused by the vertex-based method is much less frequent. Generally the constrain of independence between edges makes the task of making the silhouette appear completely smooth a challenge. If a vertex had access to both the neighbouring edge normals, using the tangent method and using the average of the two edges would allow perfectly smooth transitioning.



\section{Texture generation}

The second stage of the shader will add a watercolour fill to the objects that are being drawn. A common technique for the basis of this type of shading is cel shading. In cel shading the light values across the object are calculated as usual, but then light intensity thresholding is applied to it. This constructs separate fields with constant colour values, the further away from the light, the darker the field. Sometimes the harsh borders generated by the edges of the fields seem unnatural and there are often unsatisfying effects - for example only an area of a few pixels having a certain colour, which an artist wouldn't consider shading. Since the silhouette edges already indicate a sense of lighting, this is not necessary for this particular drawing style. To avoid these unnecessary artifacts, the whole fill will be a constant colour.

Another issue that arises is related to continuity when we actually try to texture the shape to look like paint. The trivial way to add texture to the fill of the shape would be to add a texture to the object mesh itself. This is not satisfactory - the result will not give the illusion of 2D paint, since viewing the faces at an angle will make the assigned texture appear squished. The other simple solution is take a watercolour texture spanning the whole screen and crop it to match the shape of the object. This will indeed look 2D, but if we move around the shape in the scene it will appear extremely unnatural - as if the object was a hole through which the texture is seen. This issue can be solved when the shape moves vertically, horizontally, towards and away from the camera by translating and enlarging the texture accordingly. Rotation around the object however has no trivial solution that would look natural.

The idea behind my method of shading is generating a set of key points on the 3D mesh which will act as anchor points for the texture. These points will act naturally when moving around an object as they are attached to the the surface. Now, the screen coordinates of these anchor-points can be used as parameters in the generation of a 2D texture. There are many ways to do this that would lead to interesting effects, but in this paper I will consider a basic version which simply places a 2D blob on each one of the anchor-points in the texture.

\end{document}
